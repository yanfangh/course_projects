{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import html\n",
    "def clean_text(text):\n",
    "    text = text.rstrip()\n",
    "\n",
    "    if '\"\"' in text:\n",
    "        if text[0] == text[-1] == '\"':\n",
    "            text = text[1:-1]\n",
    "        text = text.replace('\\\\\"\"', '\"')\n",
    "        text = text.replace('\"\"', '\"')\n",
    "\n",
    "    text = text.replace('\\\\\"\"', '\"')\n",
    "\n",
    "    text = html.unescape(text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "  \n",
    "def label2categories(label):\n",
    "  codetable={'positive':0, 'neutral':1, 'negative':2}\n",
    "  return codetable[label]\n",
    "\n",
    "def read_files(files):\n",
    "  data={}\n",
    "  for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "      for line in f:\n",
    "        columns=line.rstrip().split('\\t')\n",
    "        tweet_id, sentiment=columns[0], columns[1]\n",
    "        text=clean_text(\" \".join(columns[2:]))\n",
    "        data[tweet_id]=(label2categories(sentiment), text)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load traning dataset\n",
    "folder=\"/home/s2465922/tm/semeval-2017-tweets_Subtask-A/downloaded\"\n",
    "files=glob.glob(os.path.join(folder, \"*.tsv\"))\n",
    "\n",
    "data=read_files(files)\n",
    "      \n",
    "#load test dataset\n",
    "gold_file=\"/home/s2465922/tm/semeval-2017-tweets_Subtask-A/SemEval2017-task4-test.subtask-A.english.txt\"\n",
    "gold=read_files([gold_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12284"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, ratio):\n",
    "  train_ratio = ratio\n",
    "  train_split_index = int(len(dataset) * train_ratio)\n",
    "\n",
    "  train = dataset[:train_split_index]\n",
    "  test = dataset[train_split_index:]\n",
    "\n",
    "  return train, test\n",
    "\n",
    "train, val=split_data(list(data.values()), 0.9)\n",
    "X_train, y_train=list(zip(*train))[1], list(zip(*train))[0]\n",
    "X_val, y_val=list(zip(*val))[1], list(zip(*val))[0]\n",
    "\n",
    "\n",
    "X_test, y_test=list(zip(*gold.values()))[1], list(zip(*gold.values()))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2465922/miniconda3/envs/ve1/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2465922/miniconda3/envs/ve1/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor=TextPreProcessor(\n",
    "  normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "           'time', 'url', 'date', 'number'],\n",
    "  include_tags={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "                'emphasis', 'censored'},\n",
    "  fix_html=True,\n",
    "  segmenter=\"twitter\",\n",
    "  corrector=\"twitter\",\n",
    "  unpack_hashtags=True,\n",
    "  unpack_contractions=True,\n",
    "  spell_correct_elong=False,\n",
    "  tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "  dicts=[emoticons])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Naive Nayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* convert text to bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train_full=X_train+X_val\n",
    "y_train_full=y_train+y_val\n",
    "\n",
    "# preprcocess text\n",
    "X_train_preprocessed=np.array([' '.join(text_processor.pre_process_doc(sent)) for sent in X_train_full])\n",
    "X_test_preprocessed=np.array([' '.join(text_processor.pre_process_doc(sent)) for sent in X_test])\n",
    "\n",
    "#calculate tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf=TfidfVectorizer(ngram_range=(1, 3),\n",
    "                      binary=True,\n",
    "                      smooth_idf=False)\n",
    "X_train_tfidf=tf_idf.fit_transform(X_train_preprocessed)\n",
    "X_test_tfidf=tf_idf.transform(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use cross-validation to find the best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def get_auc_CV(model):\n",
    "  kf=StratifiedKFold(10, shuffle=True, random_state=1)\n",
    "  \n",
    "  #get AUC scores\n",
    "  auc=cross_val_score(model, X_train_tfidf, y_train_full, \n",
    "                      scoring=\"accuracy\", cv=kf, n_jobs=-1)\n",
    "  \n",
    "  return auc.mean()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "res=pd.Series([get_auc_CV(MultinomialNB(alpha=i))\n",
    "              for i in np.arange(1, 10, 0.1)],\n",
    "             index=np.arange(1, 10, 0.1))\n",
    "\n",
    "best_alpha = np.round(res.idxmax(), 2)\n",
    "print('Best alpha: ', best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "def evaluate_measures(y_pred, y_true):\n",
    "  y_pred=np.array(y_pred)\n",
    "  y_true=np.array(y_true)\n",
    "  accu=np.mean(y_pred==y_true)\n",
    "  \n",
    "  recs=recall_score(y_true, y_pred, average=None)\n",
    "  mean_rec=recall_score(y_true, y_pred, average='macro')\n",
    "  \n",
    "  precs=precision_score(y_true, y_pred, average=None)\n",
    "  mean_prec=precision_score(y_true, y_pred, average='macro')\n",
    "  \n",
    "  f1_pn=f1_score(y_true, y_pred, average='macro', labels=[0,2])\n",
    "  scores={'accuracy': accu, \n",
    "          'recall': {'pos': recs[0], 'neu': recs[1], 'neg': recs[2], 'mean': mean_rec},\n",
    "          'precision': {'pos': precs[0], 'neu': precs[1], 'neg': precs[2], 'mean': mean_prec},\n",
    "          'f1_pn': f1_pn}\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model=MultinomialNB(alpha=best_alpha)\n",
    "nb_model.fit(X_train_tfidf, y_train_full)\n",
    "nb_preds=np.argmax(nb_model.predict_proba(X_test_tfidf), axis=1)\n",
    "nb_scores=evaluate_measures(nb_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1228, 1147,    0],\n",
       "       [ 664, 5273,    0],\n",
       "       [ 285, 3686,    1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, nb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2697391304347826"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*0.564*0.517/(0.564+0.517)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5293064148485835,\n",
       " 'recall': {'pos': 0.5170526315789473,\n",
       "  'neu': 0.888159002863399,\n",
       "  'neg': 0.00025176233635448137,\n",
       "  'mean': 0.46848779892623355},\n",
       " 'precision': {'pos': 0.5640790078089113,\n",
       "  'neu': 0.5217692459924798,\n",
       "  'neg': 1.0,\n",
       "  'mean': 0.6952827512671304},\n",
       " 'f1_pn': 0.2700232279662767}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "Device name: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda:1\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* convert text to a list of token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "MAX_LEN=140\n",
    "\n",
    "def tokenize(data):\n",
    "  input_ids=[]\n",
    "  attention_masks=[]\n",
    "  \n",
    "  for sent in data:\n",
    "    encoded_sent=tokenizer.encode_plus(\n",
    "      text=' '.join(text_processor.pre_process_doc(sent)),\n",
    "      add_special_tokens=True,\n",
    "      max_length=MAX_LEN,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True\n",
    "    )\n",
    "    input_ids.append(encoded_sent.get('input_ids'))\n",
    "    attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "    \n",
    "  input_ids=torch.tensor(input_ids)\n",
    "  attention_masks=torch.tensor(attention_masks)\n",
    "  \n",
    "  return input_ids, attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#get the input to the bert model\n",
    "train_inputs, train_masks = tokenize(X_train)\n",
    "val_inputs, val_masks=tokenize(X_val)\n",
    "\n",
    "#convert other data types to torch.Tensor\n",
    "train_labels=torch.tensor(y_train)\n",
    "val_labels=torch.tensor(y_val)\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "#create the dataloader for the training set\n",
    "train_data=TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler=RandomSampler(train_data)\n",
    "train_dataloader=DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "#create the dataloader for the validation set\n",
    "val_data=TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler=RandomSampler(val_data)\n",
    "val_dataloader=DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fine-tune bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "  def __init__(self, freeze_bert=False):\n",
    "    super(BertClassifier, self).__init__()\n",
    "    #specify hidden size of BERT, hidden size or classifier, and number of labels\n",
    "    D_in, H, D_out=768, 50, 3\n",
    "    \n",
    "    self.bert=BertModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    self.classifier=nn.Sequential(\n",
    "      nn.Linear(D_in, H),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(0.5),\n",
    "      nn.Linear(H, D_out)\n",
    "    )\n",
    "    \n",
    "    if freeze_bert:\n",
    "      for param in self.bert.parameters():\n",
    "        param.requires_grad=False\n",
    "        \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    outputs=self.bert(input_ids=input_ids,\n",
    "                     attention_mask=attention_mask)\n",
    "    last_hidden_state_cls=outputs[0][:,0,:]\n",
    "    logits=self.classifier(last_hidden_state_cls)\n",
    "\n",
    "    return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "  bert_classifier=BertClassifier(freeze_bert=False)\n",
    "  \n",
    "  bert_classifier.to(device)\n",
    "  \n",
    "  optimizer=AdamW(bert_classifier.parameters(),\n",
    "                 lr=1e-6,\n",
    "                 eps=1e-8)\n",
    "  \n",
    "  total_steps=len(train_dataloader)*epochs\n",
    "  \n",
    "  scheduler=get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps=0,\n",
    "                                           num_training_steps=total_steps)\n",
    "  return bert_classifier, optimizer, scheduler\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=34):\n",
    "  random.seed(seed_value)\n",
    "  np.random.seed(seed_value)\n",
    "  torch.manual_seed(seed_value)\n",
    "  torch.cuda.manual_seed_all(seed_value)\n",
    "  \n",
    "def train(model, train_dataloader, val_dataloade=None, epochs=4, evaluation=False):\n",
    "  print(\"Starting training...\\n\")\n",
    "  \n",
    "  for epoch_i in range(epochs):\n",
    "    print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    #Measure the elapsed time of each epoch\n",
    "    t0_epoch, t0_batch=time.time(), time.time()\n",
    "    \n",
    "    total_loss, batch_loss, batch_counts=0, 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "      batch_counts+=1\n",
    "      b_input_ids, b_attn_mask, b_labels=tuple(t.to(device) for t in batch)\n",
    "      \n",
    "      model.zero_grad()\n",
    "      \n",
    "      logits=model(b_input_ids, b_attn_mask)\n",
    "      \n",
    "      loss=loss_fn(logits, b_labels)\n",
    "      batch_loss+=loss.item()\n",
    "      total_loss+=loss.item()\n",
    "      \n",
    "      loss.backward()\n",
    "      \n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "      \n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      \n",
    "      if (step%100==0 and step!=0) or (step==len(train_dataloader)-1):\n",
    "        time_elapsed=time.time()-t0_batch\n",
    "        \n",
    "        print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "        \n",
    "        batch_loss, batch_counts=0, 0\n",
    "        t0_batch=time.time()\n",
    "        \n",
    "    avg_train_loss=total_loss/len(train_dataloader)\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    if evaluation==True:\n",
    "      val_loss, val_accuracy=evaluate(model, val_dataloader)\n",
    "                  # Print performance over the entire training data\n",
    "      time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "      print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "      print(\"-\"*70)\n",
    "      print(\"\\n\")\n",
    "    \n",
    "  print(\"Training complete!\")\n",
    "    \n",
    "def evaluate(model, val_dataloader):\n",
    "  model.eval()\n",
    "  \n",
    "  val_accuracy=[]\n",
    "  val_loss=[]\n",
    "  \n",
    "  for batch in val_dataloader:\n",
    "    #load batch to gpu\n",
    "    b_input_ids, b_attn_mask, b_labels=tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    #compute logits\n",
    "    with torch.no_grad():\n",
    "      logits=model(b_input_ids, b_attn_mask)\n",
    "    \n",
    "    #compute loss\n",
    "    loss=loss_fn(logits, b_labels)\n",
    "    val_loss.append(loss.item())\n",
    "    \n",
    "    #get the predictions\n",
    "    preds=torch.argmax(logits, dim=1).flatten()\n",
    "    \n",
    "    #calculate the accuracy rate\n",
    "    accuracy=(preds==b_labels).cpu().numpy().mean()*100\n",
    "    val_accuracy.append(accuracy)\n",
    "    \n",
    "  #compute the average accuracy and loss over the validation set\n",
    "  val_loss=np.mean(val_loss)\n",
    "  val_accuracy=np.mean(val_accuracy)\n",
    "  \n",
    "  return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   100   |   1.079502   |     -      |     -     |   39.93  \n",
      "   1    |   200   |   1.030324   |     -      |     -     |   39.81  \n",
      "   1    |   300   |   1.028618   |     -      |     -     |   40.05  \n",
      "   1    |   400   |   1.004597   |     -      |     -     |   40.14  \n",
      "   1    |   500   |   0.997979   |     -      |     -     |   40.19  \n",
      "   1    |   600   |   0.974735   |     -      |     -     |   40.24  \n",
      "   1    |   700   |   0.954767   |     -      |     -     |   40.10  \n",
      "   1    |   800   |   0.950345   |     -      |     -     |   40.29  \n",
      "   1    |   900   |   0.911628   |     -      |     -     |   39.83  \n",
      "   1    |  1000   |   0.893920   |     -      |     -     |   42.85  \n",
      "   1    |  1100   |   0.845568   |     -      |     -     |   44.10  \n",
      "   1    |  1200   |   0.843783   |     -      |     -     |   45.01  \n",
      "   1    |  1300   |   0.828069   |     -      |     -     |   44.34  \n",
      "   1    |  1394   |   0.810953   |     -      |     -     |   41.39  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.940281   |  0.793654  |   61.48   |  596.54  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   100   |   0.807370   |     -      |     -     |   45.05  \n",
      "   2    |   200   |   0.809177   |     -      |     -     |   40.06  \n",
      "   2    |   300   |   0.798000   |     -      |     -     |   38.59  \n",
      "   2    |   400   |   0.777747   |     -      |     -     |   38.97  \n",
      "   2    |   500   |   0.789696   |     -      |     -     |   38.71  \n",
      "   2    |   600   |   0.777048   |     -      |     -     |   38.54  \n",
      "   2    |   700   |   0.773195   |     -      |     -     |   38.85  \n",
      "   2    |   800   |   0.768772   |     -      |     -     |   40.38  \n",
      "   2    |   900   |   0.761128   |     -      |     -     |   40.25  \n",
      "   2    |  1000   |   0.761144   |     -      |     -     |   40.38  \n",
      "   2    |  1100   |   0.758681   |     -      |     -     |   40.37  \n",
      "   2    |  1200   |   0.731875   |     -      |     -     |   40.43  \n",
      "   2    |  1300   |   0.754293   |     -      |     -     |   40.50  \n",
      "   2    |  1394   |   0.755418   |     -      |     -     |   37.85  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.773211   |  0.717571  |   67.58   |  573.52  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   100   |   0.735891   |     -      |     -     |   39.90  \n",
      "   3    |   200   |   0.730962   |     -      |     -     |   39.63  \n",
      "   3    |   300   |   0.732856   |     -      |     -     |   39.74  \n",
      "   3    |   400   |   0.747268   |     -      |     -     |   42.20  \n",
      "   3    |   500   |   0.734850   |     -      |     -     |   44.98  \n",
      "   3    |   600   |   0.717356   |     -      |     -     |   44.93  \n",
      "   3    |   700   |   0.738643   |     -      |     -     |   45.56  \n",
      "   3    |   800   |   0.724296   |     -      |     -     |   46.30  \n",
      "   3    |   900   |   0.716251   |     -      |     -     |   41.20  \n",
      "   3    |  1000   |   0.716027   |     -      |     -     |   40.31  \n",
      "   3    |  1100   |   0.711183   |     -      |     -     |   40.00  \n",
      "   3    |  1200   |   0.704880   |     -      |     -     |   40.24  \n",
      "   3    |  1300   |   0.711083   |     -      |     -     |   39.91  \n",
      "   3    |  1394   |   0.712231   |     -      |     -     |   37.63  \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.723900   |  0.696788  |   68.76   |  597.08  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   100   |   0.706670   |     -      |     -     |   40.58  \n",
      "   4    |   200   |   0.727441   |     -      |     -     |   40.33  \n",
      "   4    |   300   |   0.699906   |     -      |     -     |   40.30  \n",
      "   4    |   400   |   0.703256   |     -      |     -     |   40.31  \n",
      "   4    |   500   |   0.712857   |     -      |     -     |   40.29  \n",
      "   4    |   600   |   0.699856   |     -      |     -     |   40.30  \n",
      "   4    |   700   |   0.699266   |     -      |     -     |   40.27  \n",
      "   4    |   800   |   0.705910   |     -      |     -     |   40.27  \n",
      "   4    |   900   |   0.702311   |     -      |     -     |   40.15  \n",
      "   4    |  1000   |   0.694928   |     -      |     -     |   40.28  \n",
      "   4    |  1100   |   0.697011   |     -      |     -     |   40.26  \n",
      "   4    |  1200   |   0.710677   |     -      |     -     |   40.27  \n",
      "   4    |  1300   |   0.701867   |     -      |     -     |   40.31  \n",
      "   4    |  1394   |   0.711538   |     -      |     -     |   37.78  \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.705223   |  0.684691  |   69.13   |  576.26  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   100   |   0.700744   |     -      |     -     |   40.83  \n",
      "   5    |   200   |   0.679574   |     -      |     -     |   40.30  \n",
      "   5    |   300   |   0.688973   |     -      |     -     |   40.37  \n",
      "   5    |   400   |   0.705795   |     -      |     -     |   40.38  \n",
      "   5    |   500   |   0.702067   |     -      |     -     |   40.32  \n",
      "   5    |   600   |   0.686987   |     -      |     -     |   40.27  \n",
      "   5    |   700   |   0.684554   |     -      |     -     |   40.27  \n",
      "   5    |   800   |   0.713667   |     -      |     -     |   40.26  \n",
      "   5    |   900   |   0.702940   |     -      |     -     |   40.29  \n",
      "   5    |  1000   |   0.699869   |     -      |     -     |   40.26  \n",
      "   5    |  1100   |   0.675645   |     -      |     -     |   40.27  \n",
      "   5    |  1200   |   0.705076   |     -      |     -     |   40.24  \n",
      "   5    |  1300   |   0.698770   |     -      |     -     |   40.31  \n",
      "   5    |  1394   |   0.704363   |     -      |     -     |   37.66  \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.696328   |  0.683602  |   69.21   |  576.59  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(34)\n",
    "bert_classifier, optimizer, scheduler=initialize_model(epochs=5)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=5, evaluation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict(model, test_dataloader):\n",
    "  model.eval()\n",
    "  \n",
    "  preds=[]\n",
    "  \n",
    "  for batch in test_dataloader:\n",
    "    #load batch to gpu\n",
    "    b_input_ids, b_attn_mask=tuple(t.to(device) for t in batch)[:2]\n",
    "    \n",
    "    #compute logits\n",
    "    with torch.no_grad():\n",
    "      logits=model(b_input_ids, b_attn_mask)\n",
    "    \n",
    "    #get the predictions\n",
    "    preds.append(torch.argmax(logits, dim=1).flatten())\n",
    "\n",
    "  preds=torch.cat(preds, dim=0).cpu().numpy()\n",
    "\n",
    "  return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "print('Tokenizing data...')\n",
    "test_inputs, test_masks = tokenize([' '.join(text_processor.pre_process_doc(sent)) for sent in X_test])\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preds=bert_predict(bert_classifier, test_dataloader)\n",
    "bert_scores=evaluate_measures(bert_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1695,  623,   57],\n",
       "       [ 807, 4294,  836],\n",
       "       [ 163, 1292, 2517]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, bert_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6924454575056985,\n",
       " 'recall': {'pos': 0.7136842105263158,\n",
       "  'neu': 0.7232609061815732,\n",
       "  'neg': 0.6336858006042296,\n",
       "  'mean': 0.6902103057707062},\n",
       " 'precision': {'pos': 0.6360225140712945,\n",
       "  'neu': 0.6915767434369464,\n",
       "  'neg': 0.7381231671554253,\n",
       "  'mean': 0.6885741415545553},\n",
       " 'f1_pn': 0.6772740320728672}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
