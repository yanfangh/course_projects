{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the source codes and comments of retriever and reader module for question-answering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "\n",
    "import os \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from schema import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cluster mode\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"sddm\") \\\n",
    "    .master(\"spark://132.229.44.220:7272\") \\\n",
    "    .config(\"spark.driver.memory\", \"50g\")\\\n",
    "    .config(\"spark.executor.memory\",'10g') \\\n",
    "    .config(\"spark.memory.storageFraction\", 0.1)\\\n",
    "    .config(\"spark.ui.port\",4077)\\\n",
    "    .config(\"spark.driver.maxResultSize\",\"5g\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", 600)\\\n",
    "    .config(\"spark.network.timeout\", 1000)\\\n",
    "    .config('spark.local.dir', '/home/s2465922/tmp')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# # local mode\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"sddm\") \\\n",
    "#     .master(\"local[20]\") \\\n",
    "#     .config(\"spark.driver.memory\", \"50g\")\\\n",
    "#     .config(\"spark.driver.maxResultSize\",\"10g\")\\\n",
    "#     .config(\"spark.ui.port\",4077)\\\n",
    "#     .config('spark.local.dir', '/home/s2465922/tmp')\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# import sparknlp\n",
    "# spark=sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.uiWebUrl\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "#### read metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/data/s2465922/cord_19/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata\n",
    "metadata=spark.read.csv(path+'metadata.csv',header=True,inferSchema=True,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138794"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data with full text\n",
    "metadata.createOrReplaceTempView('md')\n",
    "meta_with_text=spark.sql('select * from md where pdf_json_files is not null or pmc_json_files is not null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract filename from pmc or pdf json files\n",
    "meta_with_text=meta_with_text.withColumn('file', F.when(F.col('pmc_json_files')!='', F.col('pmc_json_files'))\\\n",
    "                                         .otherwise(F.col('pdf_json_files')))\n",
    "\n",
    "# remove records that have incorrect filenames \n",
    "meta_with_text.createOrReplaceTempView('mwt')\n",
    "meta_with_text=spark.sql('select * from mwt where file RLIKE \"^document_parses\"')\\\n",
    "              .dropDuplicates(subset=[\"file\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta_with_text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames\n",
    "filenames=meta_with_text.select('file').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# get complete file path\n",
    "fs=[]\n",
    "for f in filenames:\n",
    "  fs.extend(f.split('; '))\n",
    "  \n",
    "filepath=[path+f for f in set(fs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read body_text data\n",
    "texts=spark.read.schema(schema).json(filepath, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- paper_id: string (nullable = true)\n",
      " |-- abstract: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |-- body_text: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |    |    |-- section: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# texts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract paper_id and text part of body_text\n",
    "texts.createOrReplaceTempView(\"papers\")\n",
    "ts=spark.sql('select paper_id,  concat_ws(\"\\n\",body_text.text) as text from papers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* common data preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text2paragraph(text):\n",
    "  return re.split('\\n+', text)\n",
    "\n",
    "def paragraph2sentence(paragraph):\n",
    "  return [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", paragraph) if s != \"\"]\n",
    "\n",
    "def text2sentence(text):\n",
    "  sentences=[]\n",
    "  for paragraph in text2paragraph(text):\n",
    "    sentences.extend(paragraph2sentence(paragraph))\n",
    "  return sentences\n",
    "\n",
    "text2sentence_udf=F.udf(lambda x: text2sentence(x), ArrayType(StringType()))\n",
    "\n",
    "def sentence2tokens(sentence):\n",
    "  return re.findall(r'[^\\s,\":;()]+', sentence.strip('.?!').lower())\n",
    "  \n",
    "sentence2tokens_udf=F.udf(sentence2tokens, ArrayType(StringType()) )\n",
    "\n",
    "def text2tokens(text):\n",
    "  tokens=[]\n",
    "  for paragraph in text2paragraph(text):\n",
    "    for sentence in paragraph2sentence(paragraph):\n",
    "      tokens.extend(sentence2tokens(sentence))\n",
    "  return tokens\n",
    "\n",
    "text2tokens_udf=F.udf(text2tokens, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* find articles that contains covid-19 or other similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [r\"2019[\\-\\s]?n[\\-\\s]?cov\", \"2019 novel coronavirus\", \"coronavirus 2019\", r\"coronavirus disease (?:20)?19\",\n",
    "            r\"covid(?:[\\-\\s]?19)?\", r\"n\\s?cov[\\-\\s]?2019\", r\"sars-cov-?2\", r\"wuhan (?:coronavirus|cov|pneumonia)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def covid_tag(text):\n",
    "  for k in keywords:\n",
    "    if re.search(k, text.lower()):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "covid_tag_udf=F.udf(lambda x: covid_tag(x), BooleanType())\n",
    "\n",
    "ts_covid=ts.withColumn('covid', covid_tag_udf('text'))\\\n",
    "        .select('paper_id','text')\\\n",
    "        .filter('covid==True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18683"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_covid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fit tf-idf model using text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf  pipeline\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover, CountVectorizer, IDF\n",
    "\n",
    "stopwords =StopWordsRemover.loadDefaultStopWords('english')\n",
    "sw_remover = StopWordsRemover(inputCol='tokens', outputCol='filtered', stopWords=stopwords)\n",
    "\n",
    "tf = CountVectorizer(inputCol='filtered', outputCol='tf')\n",
    "\n",
    "idf = IDF(inputCol='tf', outputCol='tfidf')\n",
    "\n",
    "td_pipeline = Pipeline(stages=[sw_remover, tf, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform each document to a list of tokens\n",
    "ts_tokenized=ts_covid.withColumn('tokens', text2tokens_udf('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 2.465 min\n"
     ]
    }
   ],
   "source": [
    "# fit tf-idf model using text data\n",
    "from time import time\n",
    "start=time()\n",
    "td_model = td_pipeline.fit(ts_tokenized)\n",
    "print('Running time: %.3f min'%((time()-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* apply tf-idf model to query and text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load questions\n",
    "qs=spark.read.csv(\"qs.csv\", header=True)\\\n",
    "  .withColumn('tokens', sentence2tokens_udf('query'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tf-idf model to query\n",
    "qs_td=td_model.transform(qs)\n",
    "qtf_lst=[u[0] for u in qs_td.select('tf').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tf-idf model to text data\n",
    "ts_td = td_model.transform(ts_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* retrieve the top 100 papers with the  highest tf-idf scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 6.671 min\n"
     ]
    }
   ],
   "source": [
    "from time import time \n",
    "start=time()\n",
    "\n",
    "candidate_papers=[]\n",
    "for i in range(len(qtf_lst)):\n",
    "  q_tf=qtf_lst[i]\n",
    "  \n",
    "  # compute the tf-idf scores of all papers for the given query\n",
    "  get_score_udf=F.udf(lambda x: float(x.dot(q_tf)), FloatType())\n",
    "  ts_score=ts_td.withColumn('score', get_score_udf('tfidf'))\n",
    "\n",
    "  # select the top 100 papers with the highest score\n",
    "  candidate=ts_score.select('paper_id').orderBy('score',ascending=False).take(100)\n",
    "  candidate_papers.append([x[0] for x in candidate])\n",
    "  ts_score.unpersist(blocking = True)\n",
    "  \n",
    "print('Running time: %.3f min'%((time()-start)/60))\n",
    "\n",
    "# save candidate papers into the disk\n",
    "import pickle\n",
    "with open('candidate_papers_0718', 'wb') as fp:\n",
    "    pickle.dump(candidate_papers, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read candidate papers\n",
    "import pickle\n",
    "with open ('candidate_papers_0718', 'rb') as fp:\n",
    "    candidate_papers = pickle.load(fp)\n",
    "\n",
    "# get filenames of candidate papers\n",
    "filepath=[]\n",
    "for i in range(len(candidate_papers)):\n",
    "  f=[]\n",
    "  for x in candidate_papers[i]:\n",
    "    if x.startswith(\"PMC\"):\n",
    "      f.append(path+\"document_parses/pmc_json/\"+x+\".xml.json\")\n",
    "    else:\n",
    "      f.append(path+\"document_parses/pdf_json/\"+x+\".json\")\n",
    "  filepath.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract keywords from query\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopwords =StopWordsRemover.loadDefaultStopWords('english')\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer() \n",
    "\n",
    "import pandas as pd\n",
    "qs_tokens=pd.read_csv(\"qs.csv\")['query'].apply(sentence2tokens).tolist()\n",
    "\n",
    "qs_kw=[]\n",
    "for qtokens in qs_tokens:\n",
    "  q_kw=[ps.stem(x) for x in qtokens if x not in stopwords] \n",
    "  qs_kw.append(q_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_udf=F.udf(lambda xs: [ps.stem(x) for x in xs], ArrayType(StringType()))\n",
    "\n",
    "def is_question(sentence):\n",
    "  '''\n",
    "  identify whether a sentence is a question\n",
    "  '''\n",
    "  return sentence.endswith('?')\n",
    "\n",
    "is_question_udf=F.udf(is_question, BooleanType())\n",
    "\n",
    "def is_match(pg,q_kw):\n",
    "  '''\n",
    "  Input: a paragraph, the query keywords\n",
    "  Return: True if the paragraph contained any query keyword, otherwise false\n",
    "  '''\n",
    "  for q in q_kw:\n",
    "    if q in pg:\n",
    "      return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text and split text into sentences\n",
    "\n",
    "for i in range(10):\n",
    "  texts_k=spark.read.schema(schema).json(filepath[i], multiLine=True)\n",
    "\n",
    "  # filter out paragraph that do not contain any keyword of question\n",
    "  is_match_udf=F.udf(lambda x: is_match(x, qs_kw[i]), BooleanType())\n",
    "  \n",
    "  pg_k=texts_k.withColumn('paragraph', F.explode('body_text.text'))\\\n",
    "        .withColumn('pg_tokens', text2tokens_udf('paragraph'))\\\n",
    "        .withColumn('pg_token_stem', stem_udf('pg_tokens'))\\\n",
    "        .select('paper_id', 'paragraph')\\\n",
    "        .filter(is_match_udf('pg_token_stem')==True)\n",
    "\n",
    "  # split paragraph into sentences\n",
    "  # remove sentences ending with ?\n",
    "  # remove sentences shorter than 7\n",
    "  ss_k=pg_k.withColumn('sentences', text2sentence_udf('paragraph'))\\\n",
    "      .withColumn('sentence', F.explode('sentences'))\\\n",
    "      .withColumn('tokens', sentence2tokens_udf('sentence'))\\\n",
    "      .select('paper_id', 'sentence')\\\n",
    "      .filter((is_question_udf('sentence')==False) &\n",
    "              (F.size('tokens')>=7))\n",
    "  \n",
    "  ss_k.rdd.map(lambda x: '\\t'.join([x[0],x[1]])).saveAsTextFile(\"ss\"+str(i)+\".txt\")\n",
    "  spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BlueBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load BlueBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import BertConfig\n",
    "import torch\n",
    "\n",
    "bluebert_dir='/data/s2465922/NCBI_BERT_pubmed_mimic_uncased_L-12_H-768_A-12'\n",
    "# load BlueBERT tokenizer\n",
    "bluebert_tokenizer = BertTokenizer.from_pretrained(bluebert_dir)\n",
    "# load BlueBERT model\n",
    "bluebert_config = BertConfig.from_json_file(bluebert_dir+'/bert_config.json')\n",
    "bluebert_model = BertModel.from_pretrained(bluebert_dir, config=bluebert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def to_tokens(sentence):\n",
    "  '''\n",
    "  input: a sentence\n",
    "  return: a list of tokens\n",
    "  '''\n",
    "  tokens=sentence2tokens(sentence)\n",
    "  token_ids=bluebert_tokenizer.encode(tokens, add_special_tokens=False)\n",
    "  return token_ids\n",
    "\n",
    "def fix_len(x, max_len=128):\n",
    "  '''\n",
    "  input: a list of tokens\n",
    "  return: a list of tokens with a fixed length\n",
    "  '''\n",
    "  fixed=[]\n",
    "  if len(x)<max_len:\n",
    "    fixed=x+[0]*(max_len-len(x))\n",
    "  else:\n",
    "    fixed=x[:max_len]\n",
    "  return fixed\n",
    "\n",
    "def bluebert_embed(batch):\n",
    "  '''\n",
    "  input: number of sentences * token ids with fixed length\n",
    "  return: number of sentences * average embedding of words in a sentence\n",
    "  '''\n",
    "  batch=np.array(batch)\n",
    "  attention_mask=np.where(batch != 0, 1, 0)\n",
    "  outputs=bluebert_model(torch.tensor(batch), torch.tensor(attention_mask))\n",
    "  embedding=outputs[0].mean(1).detach().numpy()\n",
    "  return embedding\n",
    "\n",
    "def predict_similarity(batch, q):\n",
    "  '''\n",
    "  input: 1. number of sentences * token ids with fixed length,\n",
    "  2. sentence embedding of the given query\n",
    "  return: an array of cosine similarity \n",
    "  '''\n",
    "  q=q.reshape(1,-1)\n",
    "  batch_emb=bluebert_embed(batch)\n",
    "  return cosine_similarity(batch_emb, q).flatten()\n",
    "\n",
    "def row_preprocess(row):\n",
    "  return row.split('\\t')\n",
    "\n",
    "def row_postprocess(row, sim):\n",
    "   return '\\t'.join([row[0], row[1], format(sim, '.3f')])\n",
    "  \n",
    "def mapPartition_func(partition, q_emb, batch_size=32, threshold=0.72):\n",
    "  '''\n",
    "  This function is used to process each partition for predicting the cosine similarity \n",
    "  between each sentence and the given query.\n",
    "  '''\n",
    "  batch_rows, batch=[], []\n",
    "  count=0\n",
    "  for row in partition:\n",
    "    pre_row=row_preprocess(row)\n",
    "    batch_rows.append(pre_row)\n",
    "    batch.append(fix_len(to_tokens(pre_row[1])))\n",
    "    count+=1\n",
    "    if count==batch_size:\n",
    "      sims=predict_similarity(batch, q_emb)\n",
    "      for i in range(len(sims)):\n",
    "        if sims[i]>=threshold:\n",
    "          yield row_postprocess(batch_rows[i], sims[i])\n",
    "      count=0\n",
    "      batch_rows, batch=[], []\n",
    "      \n",
    "  if count!=0:\n",
    "    sims=predict_similarity(batch, q_emb)\n",
    "    for i in range(len(sims)):\n",
    "      if sims[i]>=threshold:\n",
    "        yield row_postprocess(batch_rows[i], sims[i])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* query processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "qs_df=pd.read_csv('qs.csv')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# convert each sentence to a sequence of tokens\n",
    "qs_tokenized=qs_df['query'].apply(lambda x: fix_len(to_tokens(x), max_len=50))\n",
    "\n",
    "# get the bluebert embeddings for each sentence\n",
    "qs_emb=bluebert_embed(qs_tokenized.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BlueBERT model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/s2465922/env/lib64/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 99.648 min\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start=time()\n",
    "\n",
    "sc=spark.sparkContext\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "for i in range(10):\n",
    "  ss_k_sim=sc.textFile('ss'+str(i)+\".txt\")\\\n",
    "          .mapPartitions(lambda x: mapPartition_func(x, qs_emb[i]))\n",
    "\n",
    "  ss_k_sim.saveAsTextFile(\"result\"+str(i))\n",
    "  spark.catalog.clearCache()\n",
    "  \n",
    "print('Running time: %.3f min'%((time()-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* process the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sc=spark.sparkContext\n",
    "res=[]\n",
    "\n",
    "# load the result\n",
    "for i in range(10):\n",
    "  res_i=sc.textFile('result'+str(i)).map(lambda x: x.split('\\t')).collect()\n",
    "\n",
    "  sims=np.array([x[2] for x in res_i]).astype(float)\n",
    "  ix=sims.argsort()[::-1][:20]\n",
    "\n",
    "  # get the bluebert embedding of top 20 sentences\n",
    "  for j in ix:\n",
    "    res.append((i, res_i[j][0], res_i[j][1], sims[j]))\n",
    "    \n",
    "# save the final result to disk\n",
    "import pandas as pd\n",
    "\n",
    "res_df=pd.DataFrame(res, columns=['query_id', 'paper_id', 'sentence', 'similarity'])\n",
    "\n",
    "token_ids=res_df['sentence'].apply(lambda x: fix_len(to_tokens(x))).tolist()\n",
    "res_df['embedding']=bluebert_embed(token_ids).tolist()\n",
    "\n",
    "res_df.to_pickle('result_bert_0722.pkl')\n",
    "res_df.to_csv('result_bert_0722.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word2vec model tranining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split document into sentence\n",
    "ss=ts.withColumn('sentences', text2sentence_udf('text'))\\\n",
    "    .withColumn('sentence', F.explode('sentences'))\\\n",
    "    .withColumn('tokens', sentence2tokens_udf('sentence'))\\\n",
    "    .select('paper_id', 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14953775"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ss.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word embedding pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover, Normalizer, Word2Vec\n",
    "stopwords = StopWordsRemover.loadDefaultStopWords('english')\n",
    "sw_remover = StopWordsRemover(inputCol='tokens', outputCol='filtered', stopWords=stopwords)\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=5,seed=56,numPartitions=10, maxIter=5, \n",
    "                    windowSize=7, inputCol = 'filtered', outputCol = 'embedding')\n",
    "normalizer=Normalizer(inputCol=\"embedding\", outputCol=\"norm_embedding\", p=2.0)\n",
    "\n",
    "w2v_pipeline=Pipeline(stages=[sw_remover, word2vec, normalizer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 132.906 min\n"
     ]
    }
   ],
   "source": [
    "# fit word2vec model \n",
    "from time import time\n",
    "start=time()\n",
    "\n",
    "w2v_model=w2v_pipeline.fit(ss)\n",
    "w2v_model.write().overwrite().save('w2v_model_0712')\n",
    "print('Running time: %.3f min'%((time()-start)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Query processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load word2vec model\n",
    "loaded_w2v=PipelineModel.load(\"w2v_model_0712\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply word2vec model to query\n",
    "\n",
    "# load questions\n",
    "qs=spark.read.csv(\"qs.csv\", header=True)\\\n",
    "  .withColumn('tokens', sentence2tokens_udf('query'))\n",
    "\n",
    "# get word2vec sentence embeddings for each query\n",
    "qs_emb=loaded_w2v.transform(qs)\n",
    "qe_lst=[u[0] for u in qs_emb.select('norm_embedding').collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word2vec model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_schema=StructType([ \n",
    "  StructField(\"paper_id\", StringType(), True),\n",
    "  StructField(\"sentence\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 6.581 min\n"
     ]
    }
   ],
   "source": [
    "# computer the cosine similarity between each sentence and query\n",
    "import pyspark.sql.functions as F\n",
    "from time import time\n",
    "\n",
    "start=time()\n",
    "result=[]\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "for i in range(len(qe_lst)):\n",
    "  \n",
    "  # get embedding vector for the given query\n",
    "  q_emb=qe_lst[i]\n",
    "\n",
    "  ss_k=spark.read.schema(ss_schema).json('ss'+str(i)+'.json')\\\n",
    "      .withColumn('tokens', sentence2tokens_udf('sentence'))\n",
    "  \n",
    "  # apply bert or word2vec model to text data\n",
    "  ss_k_emb=loaded_w2v.transform(ss_k)\n",
    "\n",
    "  # get embedding similarity between query and each sentence of papers\n",
    "  get_sim=F.udf(lambda x: float(x.dot(q_emb)), FloatType())\n",
    "  ss_k_sim=ss_k_emb.withColumn('similarity', get_sim('norm_embedding'))\n",
    "\n",
    "  # rank the similarity and obtain the top 20 sentences\n",
    "  ss_k_sim.createOrReplaceTempView(\"ssim\")\n",
    "  rank=spark.sql(\"select paper_id, sentence, embedding, similarity from ssim ORDER BY similarity DESC limit 20\")\n",
    "  \n",
    "  result.append(rank.rdd.map(tuple).collect())\n",
    "  \n",
    "  spark.catalog.clearCache()\n",
    "  \n",
    "print('Running time: %.3f min'%((time()-start)/60))\n",
    "\n",
    "# save candidate papers into the disk\n",
    "import pickle\n",
    "with open('result_w2v_0718', 'wb') as fp:\n",
    "    pickle.dump(result, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* convert the result into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename='result_w2v_0718'\n",
    "with open (filename, 'rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "    \n",
    "# create dataframe\n",
    "res=[]\n",
    "for i in range(len(data)):\n",
    "  for x in data[i]:\n",
    "    res.append([i]+list(x))\n",
    "\n",
    "res_df = pd.DataFrame(res, columns=['query_id','paper_id', 'sentence', 'embedding','similarity'])\n",
    "res_df.to_pickle(filename+'.pkl')\n",
    "res_df.to_csv(filename+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
