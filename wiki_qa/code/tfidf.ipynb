{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=pd.read_csv('wikiQA/WikiQA.tsv', sep='\\t')\n",
    "train=pd.read_csv('wikiQA/WikiQA-train.tsv', sep='\\t')\n",
    "dev=pd.read_csv('wikiQA/WikiQA-dev.tsv', sep='\\t')\n",
    "test=pd.read_csv('wikiQA/WikiQA-test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "  # delete questions without correct answers\n",
    "  ls=df.groupby(['QuestionID'])['Label'].sum()\n",
    "  ind=ls[ls==0].index\n",
    "  df=df.set_index('QuestionID').drop(ind)\n",
    "  return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=clean(texts)\n",
    "dev=clean(dev)\n",
    "test=clean(test)\n",
    "train=clean(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate all documents\n",
    "questions=pd.unique(texts['Question']).tolist()\n",
    "answers=texts['Sentence'].to_list()\n",
    "corpus=questions+answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer()\n",
    "t.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=t.texts_to_matrix(corpus, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct={}\n",
    "qlen=len(questions)\n",
    "alen=len(answers)\n",
    "\n",
    "qids=pd.unique(texts['QuestionID']).tolist()\n",
    "aids=texts['SentenceID'].to_list()\n",
    "\n",
    "for i in range(qlen):\n",
    "  ct[qids[i]]=i\n",
    "  \n",
    "for i in range(alen):\n",
    "  ct[aids[i]]=i+qlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleGenerator():\n",
    "  def __init__(self, nCandidate):\n",
    "    self.nCandidate=nCandidate\n",
    "    \n",
    "  def sample_neg_answers(self, df, QuestionID, nNegs):\n",
    "    neg_as=[]\n",
    "    negs=df[(df['QuestionID']==QuestionID) & (df['Label']==0)]['SentenceID'].to_list()\n",
    "    diff=nNegs-len(negs)\n",
    "    if diff<=0:\n",
    "      ind=np.random.choice(len(negs), nNegs, replace=False)\n",
    "      for i in ind:\n",
    "        neg_as.append(negs[i])\n",
    "    else:\n",
    "      neg_as=neg_as+negs\n",
    "      answer_pool=df[df['QuestionID']!=QuestionID]['SentenceID'].to_list()\n",
    "      ind=np.random.choice(len(answer_pool),diff,replace=False)\n",
    "      for i in ind:\n",
    "        neg_as.append(answer_pool[i]) \n",
    "    return neg_as\n",
    "        \n",
    "  def create_test_group(self, df):\n",
    "    '''\n",
    "    treat each question as a group \n",
    "    '''\n",
    "    qids=pd.unique(df['QuestionID']).tolist()\n",
    "    test_gs=[]\n",
    "    for qid in qids:\n",
    "      examples=[]\n",
    "      records=df[(df['QuestionID']==qid) & (df['Label']==1)]\n",
    "      pos_as=records['SentenceID'].tolist()\n",
    "      pos_examples=list(map(lambda p: [qid, p, int(1)], pos_as))\n",
    "      examples.extend(pos_examples)\n",
    "      neg_as=self.sample_neg_answers(df, qid, self.nCandidate-len(pos_as))\n",
    "      neg_examples=list(map(lambda n: [qid, n, int(0)], neg_as))\n",
    "      examples.extend(neg_examples)\n",
    "      test_gs.append(examples)\n",
    "    return test_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=ExampleGenerator(50)\n",
    "# gps=G.create_test_group(texts)\n",
    "\n",
    "dev_gs_for_eval=G.create_test_group(dev)\n",
    "test_gs=G.create_test_group(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a,b):\n",
    "  return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gps):\n",
    "  ranks=[]\n",
    "  aps=[]\n",
    "  for g in gps:\n",
    "    sims=[]\n",
    "    recall_pos=[]\n",
    "\n",
    "    qs, ans, labels = list(zip(*g))\n",
    "\n",
    "    # get similarities for each group\n",
    "    for i in range(len(g)):\n",
    "      ind1, ind2=ct[qs[i]], ct[ans[i]]\n",
    "      sim=cosine_sim(tfidf[ind1],tfidf[ind2])\n",
    "      sims.append(sim)\n",
    "\n",
    "    # sort the similarities desceding\n",
    "    sorted_index=np.argsort(sims)[::-1]\n",
    "\n",
    "    for i in range(len(sorted_index)):\n",
    "      if labels[sorted_index[i]]==1:\n",
    "        # record the postion of correct answers\n",
    "        recall_pos.append(i+1)\n",
    "    recall_pos=np.array(recall_pos)\n",
    "\n",
    "    # calculate the average precision\n",
    "    ap=np.mean((np.arange(len(recall_pos))+1)/recall_pos)\n",
    "    aps.append(ap)\n",
    "\n",
    "    # record the postion of first correct answer\n",
    "    ranks.append(recall_pos[0])\n",
    "\n",
    "  # calculate the mean average precision\n",
    "  MAP=sum(aps)/len(aps)\n",
    "\n",
    "  # calculate the MRR\n",
    "  ranks=np.array(ranks)\n",
    "  MRR=np.mean(1/ranks)\n",
    "\n",
    "  # calculate accuracy (precision@1)\n",
    "  ACC=sum(ranks==1)/len(ranks)\n",
    "  return MAP,MRR,ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5048147410808926, 0.5113042641615886, 0.3253968253968254)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(dev_gs_for_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5220296515084796, 0.5398578609386567, 0.3817427385892116)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP=0.51899\n",
      "MRR=0.53657\n",
      "ACC=0.36672\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP=%.5f\"%(MAP))\n",
    "print(\"MRR=%.5f\"%(MRR))\n",
    "print(\"ACC=%.5f\"%(ACC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "def count_word(sentence):\n",
    "  tokens=list(tokenize(sentence))\n",
    "  return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count=train['Sentence'].apply(count_word).tolist()\n",
    "dev_count=dev['Sentence'].apply(count_word).tolist()\n",
    "test_count=test['Sentence'].apply(count_word).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train=sum(train_count)/len(train_count)\n",
    "avg_dev=sum(dev_count)/len(dev_count)\n",
    "avg_test=sum(test_count)/len(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.952804061850912\n",
      "21.15575221238938\n",
      "21.561691113028473\n"
     ]
    }
   ],
   "source": [
    "print(avg_train)\n",
    "print(avg_dev)\n",
    "print(avg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2318"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
